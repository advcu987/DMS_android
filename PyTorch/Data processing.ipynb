{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "equal-times",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "[X] extract frames from video (.mp4)\n",
    "\n",
    "[ ] create PyTorch Dataset\n",
    "\n",
    "[ ] seaparate into training , validation and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blessed-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-czech",
   "metadata": {},
   "source": [
    "**extract frames from video and save them as .jpeg**\n",
    "\n",
    "source: https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames\n",
    "\n",
    "list of supported formats for extracted images: https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "everyday-riverside",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n"
     ]
    }
   ],
   "source": [
    "vidcap = cv2.VideoCapture('/home/advo/dev/DMS_android/PyTorch/Resources/VID_20210530_214905012.mp4')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success and count < 20:\n",
    "  cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "  success,image = vidcap.read()\n",
    "  print('Read a new frame: ', success)\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-password",
   "metadata": {},
   "source": [
    "**Implement DataLoader**\n",
    "\n",
    "Source: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pressed-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "valued-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriverFaceDataset(Dataset):\n",
    "    \"\"\"Driver Face dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with labels.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.labels_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.labels_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        if self.labels_frame.iloc[idx,1] == 1:\n",
    "            sample = {'image': image, 'driver_state': 'focused'}\n",
    "        else:\n",
    "            sample = {'image': image, 'driver_state': 'distracted'}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "burning-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_dataset = DriverFaceDataset(csv_file=\"/home/advo/dev/DMS_android/PyTorch/Resources/labels.csv\",\n",
    "                                  root_dir=\"/home/advo/dev/DMS_android/PyTorch/Resources/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "collective-franchise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distracted'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_dataset[2]['driver_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "serious-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(driver_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-bridge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
